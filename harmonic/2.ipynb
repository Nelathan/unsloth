{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from torch.nn import functional as F\n",
    "from torchvision import datasets, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.axes_grid1 import ImageGrid\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# --- Data Loading ---\n",
    "train_data = datasets.MNIST(root=\"./data\", train=True, download=True)\n",
    "test_data = datasets.MNIST(root=\"./data\", train=False, download=True)\n",
    "\n",
    "IN_FEATURES = 28 * 28\n",
    "OUT_FEATURES = 10\n",
    "BATCH_SIZE = 32\n",
    "N_EPOCH = 10\n",
    "\n",
    "LEARNING_RATE = 5e-4\n",
    "H_EXP = -4.5\n",
    "WEIGHT_DECAY = 0.002\n",
    "\n",
    "# Normalize to [-1, 1]\n",
    "Xtr = train_data.data.reshape(-1, IN_FEATURES).to(torch.float32) / 127.5 - 1.0\n",
    "Xte = test_data.data.reshape(-1, IN_FEATURES).to(torch.float32) / 127.5 - 1.0\n",
    "Ytr = train_data.targets\n",
    "Yte = test_data.targets\n",
    "\n",
    "xent = nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "# --- DistLayer Definition ---\n",
    "class DistLayer(torch.nn.Linear):\n",
    "    def __init__(self, in_features, out_features, eps=1e-4):\n",
    "        super(DistLayer, self).__init__(in_features, out_features, bias=False)\n",
    "        self.eps = eps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, N), self.weight: (V, N)\n",
    "        diff = x[:, None, :] - self.weight[None, :, :]  # (B, V, N)\n",
    "        dist_sq = torch.sum(diff * diff, dim=-1)  # (B, V)\n",
    "        dist_sq += self.eps\n",
    "        dist_sq /= torch.min(dist_sq, dim=-1, keepdim=True)[0]\n",
    "        return dist_sq\n",
    "\n",
    "\n",
    "# --- CNN Models with Extra Linear Layer ---\n",
    "# EfficientDistNet now has an extra linear layer (fc_extra) before the DistLayer.\n",
    "class EfficientDistNet(nn.Module):\n",
    "    def __init__(self, out_features, eps=1e-4):\n",
    "        super(EfficientDistNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False, groups=32)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # Global Average Pooling\n",
    "        # self.fc_extra = DistLayer(64, 64, eps=eps)\n",
    "        self.dist = DistLayer(64, out_features, eps=eps)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)  # (B, 1, 28, 28)\n",
    "        x = F.silu(self.bn1(self.conv2(F.silu(self.conv1(x)))))\n",
    "        x = F.silu(self.bn2(self.conv4(F.silu(self.conv3(x)))))\n",
    "        x = self.pool(x).view(x.shape[0], -1)  # (B, 64)\n",
    "        # x = F.silu(self.fc_extra(x))\n",
    "        return self.dist(x)\n",
    "\n",
    "\n",
    "# EfficientStandardNet now also gets an extra linear layer.\n",
    "class EfficientStandardNet(nn.Module):\n",
    "    def __init__(self, out_features):\n",
    "        super(EfficientStandardNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1, bias=False)\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(32)\n",
    "\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1, bias=False, groups=32)\n",
    "        self.conv4 = nn.Conv2d(64, 64, kernel_size=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d(1)  # Global Average Pooling\n",
    "        # self.fc_extra = nn.Linear(64, 64)      # NEW: Extra linear layer\n",
    "        self.fc = nn.Linear(64, out_features)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 1, 28, 28)  # (B, 1, 28, 28)\n",
    "        x = F.silu(self.bn1(self.conv2(F.silu(self.conv1(x)))))\n",
    "        x = F.silu(self.bn2(self.conv4(F.silu(self.conv3(x)))))\n",
    "        x = self.pool(x).view(x.shape[0], -1)  # (B, 64)\n",
    "        # x = F.silu(self.fc_extra(x))\n",
    "        return self.fc(x)\n",
    "\n",
    "\n",
    "# --- Visualization Functions ---\n",
    "# PCA on DistLayer prototypes remains as-is.\n",
    "def visualize_dist_pca(model):\n",
    "    for layer in model.modules():\n",
    "        if isinstance(layer, DistLayer):\n",
    "            weights = layer.weight.detach().cpu().numpy()\n",
    "            pca = PCA(n_components=2)\n",
    "            reduced_weights = pca.fit_transform(weights)\n",
    "\n",
    "            plt.figure(figsize=(8, 6))\n",
    "            plt.scatter(\n",
    "                reduced_weights[:, 0], reduced_weights[:, 1], c=range(10), cmap=\"tab10\"\n",
    "            )\n",
    "            plt.colorbar(label=\"Digit Class\")\n",
    "            plt.xlabel(\"Principal Component 1\")\n",
    "            plt.ylabel(\"Principal Component 2\")\n",
    "            plt.title(\"PCA of DistLayer Prototypes\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "# Old visualization that multiplies weight matrices works only for pure linear layers.\n",
    "# We now add a CNN-specific version using a gradient-based saliency map.\n",
    "def visualize_activation_maps(model, in_features, out_features):\n",
    "    # If the model uses any conv layers, use the CNN visualization.\n",
    "    if any(isinstance(m, nn.Conv2d) for m in model.modules()):\n",
    "        visualize_activation_maps_cnn(model)\n",
    "    else:\n",
    "        # Fallback (not expected to hit this branch for our CNNs)\n",
    "        combined_weights = None\n",
    "        for layer in model.modules():\n",
    "            if isinstance(layer, nn.Linear) and not isinstance(layer, DistLayer):\n",
    "                if combined_weights is None:\n",
    "                    combined_weights = layer.weight\n",
    "                else:\n",
    "                    combined_weights = torch.matmul(layer.weight, combined_weights)\n",
    "            elif isinstance(layer, DistLayer):\n",
    "                combined_weights = torch.matmul(layer.weight, combined_weights)\n",
    "        if combined_weights is not None:\n",
    "            fig = plt.figure(figsize=(15, 3))\n",
    "            ig = ImageGrid(fig, 111, (1, out_features))\n",
    "            for i, ax in enumerate(ig.axes_all):\n",
    "                ax.axis(\"off\")\n",
    "                image = (\n",
    "                    combined_weights[i]\n",
    "                    .reshape(int(np.sqrt(in_features)), int(np.sqrt(in_features)))\n",
    "                    .cpu()\n",
    "                    .numpy()\n",
    "                )\n",
    "                lim = np.abs(image).max()\n",
    "                ax.imshow(image, vmin=-lim, vmax=lim, cmap=\"coolwarm\")\n",
    "                ax.set_title(f\"Digit {i}\")\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "def visualize_activation_maps_cnn(model, num_classes=10):\n",
    "    \"\"\"\n",
    "    Uses a dummy input and computes gradients of each class output with respect\n",
    "    to the input image. The resulting \"saliency\" map shows the influence of each pixel.\n",
    "    \"\"\"\n",
    "    # model.eval()\n",
    "    # Use a baseline input; zeros work fine given our normalization (-1 to 1)\n",
    "    dummy_input = torch.zeros((1, 1, 28, 28), requires_grad=True).cuda()\n",
    "    fig, axes = plt.subplots(1, num_classes, figsize=(num_classes * 2, 2))\n",
    "    for i in range(num_classes):\n",
    "        model.zero_grad()\n",
    "        if dummy_input.grad is not None:\n",
    "            dummy_input.grad.zero_()\n",
    "        output = model(dummy_input)\n",
    "        # For DistNet the lower the distance, the stronger the match.\n",
    "        score = -output[0, i] if hasattr(model, \"dist\") else output[0, i]\n",
    "        score.backward(retain_graph=True)\n",
    "        # Take the maximum gradient across the channel dimension (should be 1 channel anyway)\n",
    "        saliency, _ = torch.max(dummy_input.grad.data.abs(), dim=1)\n",
    "        saliency = saliency.squeeze().cpu().numpy()\n",
    "        axes[i].imshow(saliency, cmap=\"hot\")\n",
    "        axes[i].set_title(f\"Class {i}\")\n",
    "        axes[i].axis(\"off\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # model.train()\n",
    "\n",
    "\n",
    "# --- Training Function ---\n",
    "def train_and_evaluate(\n",
    "    model, optimizer, criterion, harmonic_exponent=None, visualize=False, model_name=\"\"\n",
    "):\n",
    "    scaler = torch.amp.GradScaler(\"cuda\")\n",
    "    losses = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    for ep in range(N_EPOCH):\n",
    "        # Cosine learning rate schedule\n",
    "        lr = LEARNING_RATE * np.cos(ep / N_EPOCH * np.pi / 2)\n",
    "        for group in optimizer.param_groups:\n",
    "            group[\"lr\"] = lr\n",
    "\n",
    "        epoch_losses = []\n",
    "        indices = torch.randperm(len(Xtr))\n",
    "        for b0 in range(0, len(Xtr), BATCH_SIZE):\n",
    "            x = Xtr[indices[b0 : b0 + BATCH_SIZE]].cuda()\n",
    "            y = Ytr[indices[b0 : b0 + BATCH_SIZE]].cuda()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            with torch.amp.autocast(\"cuda\", dtype=torch.bfloat16):\n",
    "                if harmonic_exponent is not None:  # For DistNet\n",
    "                    ds = model(x)\n",
    "                    dinv = ds**harmonic_exponent\n",
    "                    probs = dinv / torch.sum(dinv, dim=-1, keepdim=True)\n",
    "                    logits = -torch.log(probs)\n",
    "                    loss = logits[range(y.size(0)), y].mean()\n",
    "                else:  # StandardNet\n",
    "                    logits = model(x)\n",
    "                    loss = criterion(logits, y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "            epoch_losses.append(loss.item())\n",
    "\n",
    "        losses.append(np.mean(epoch_losses))\n",
    "\n",
    "        corrects = []\n",
    "        for i0 in range(0, len(Xte), 128):\n",
    "            with torch.no_grad():\n",
    "                if harmonic_exponent is not None:\n",
    "                    preds = model(Xte[i0 : i0 + 128].cuda()).argmin(\n",
    "                        1\n",
    "                    )  # DistNet: lower distance â†’ better match\n",
    "                else:\n",
    "                    preds = model(Xte[i0 : i0 + 128].cuda()).argmax(1)\n",
    "                corrects.extend((preds == Yte[i0 : i0 + 128].cuda()).cpu().tolist())\n",
    "        test_acc = np.mean(corrects)\n",
    "        test_accuracies.append(test_acc)\n",
    "\n",
    "        lr = optimizer.param_groups[0][\"lr\"]\n",
    "        print(\n",
    "            f\"{model_name} - After {ep+1} ep (lr={lr:.2e}): avg loss {np.mean(epoch_losses):.2e} ; test acc {test_acc:.2%} (error: {1-test_acc:.2%})\"\n",
    "        )\n",
    "\n",
    "    if visualize:\n",
    "        # For CNN models, our new saliency-based activation maps work regardless of harmonic_exponent.\n",
    "        visualize_dist_pca(model)\n",
    "        visualize_activation_maps(model, IN_FEATURES, OUT_FEATURES)\n",
    "\n",
    "    return losses, test_accuracies\n",
    "\n",
    "\n",
    "# --- DistNet (CNN with DistLayer) ---\n",
    "print(\"\\n--- Training CNN DistNet ---\")\n",
    "dist_model = EfficientDistNet(OUT_FEATURES).cuda()\n",
    "dist_optimizer = torch.optim.AdamW(\n",
    "    dist_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "dist_losses, dist_accuracies = train_and_evaluate(\n",
    "    dist_model,\n",
    "    dist_optimizer,\n",
    "    None,\n",
    "    harmonic_exponent=H_EXP,\n",
    "    visualize=True,\n",
    "    model_name=\"CNN DistNet\",\n",
    ")\n",
    "\n",
    "# --- StandardNet (CNN with final fc) ---\n",
    "print(\"\\n--- Training CNN StandardNet ---\")\n",
    "standard_model = EfficientStandardNet(OUT_FEATURES).cuda()\n",
    "standard_optimizer = torch.optim.AdamW(\n",
    "    standard_model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY\n",
    ")\n",
    "standard_losses, standard_accuracies = train_and_evaluate(\n",
    "    standard_model,\n",
    "    standard_optimizer,\n",
    "    xent,\n",
    "    harmonic_exponent=None,\n",
    "    visualize=True,\n",
    "    model_name=\"CNN StandardNet\",\n",
    ")\n",
    "\n",
    "# --- Plot Results ---\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 6))\n",
    "\n",
    "axes[0].plot(dist_losses, label=\"CNN DistNet\")\n",
    "axes[0].plot(standard_losses, label=\"CNN StandardNet\")\n",
    "axes[0].set_title(\"Training Loss\")\n",
    "axes[0].set_xlabel(\"Epoch\")\n",
    "axes[0].set_ylabel(\"Loss\")\n",
    "axes[0].legend()\n",
    "\n",
    "axes[1].plot(dist_accuracies, label=\"CNN DistNet\")\n",
    "axes[1].plot(standard_accuracies, label=\"CNN StandardNet\")\n",
    "axes[1].set_title(\"Test Accuracy\")\n",
    "axes[1].set_xlabel(\"Epoch\")\n",
    "axes[1].set_ylabel(\"Accuracy\")\n",
    "axes[1].legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Print Summary ---\n",
    "print(\"\\n--- Summary ---\")\n",
    "print(\n",
    "    f\"CNN DistNet: Best Accuracy: {max(dist_accuracies):.4f}, Final Loss: {dist_losses[-1]:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"CNN StandardNet: Best Accuracy: {max(standard_accuracies):.4f}, Final Loss: {standard_losses[-1]:.4f}\"\n",
    ")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "unsloth",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
